{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPx0y9t36fB/shI2WqO0ar0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krititiwari31-rgb/Fake-Review-Detection/blob/main/Fake_Review_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (if not already there)\n",
        "!pip install pandas matplotlib textblob --quiet\n",
        "\n",
        "# Import Python libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n"
      ],
      "metadata": {
        "id": "nM-ZsEXxS2bQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"synthetic_amazon_reviews.csv\")\n",
        "\n",
        "# See first few rows\n",
        "df.head()\n",
        "\n",
        "# Check basic info\n",
        "df.info()\n",
        "\n",
        "# See how many reviews, products, users, etc.\n",
        "print(\"Total reviews:\", len(df))\n",
        "print(\"Unique products:\", df['productID'].nunique())\n",
        "print(\"Unique users:\", df['reviewerID'].nunique())\n",
        "\n",
        "# Rating distribution\n",
        "df['rating'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Rating Distribution')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "slKOny5qsnKy",
        "outputId": "c095c803-9e57-4ada-dddc-3aa7456879e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'synthetic_amazon_reviews.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3699759056.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"synthetic_amazon_reviews.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# See first few rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'synthetic_amazon_reviews.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9vyctl0F8ll6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values\n",
        "df['reviewText'] = df['reviewText'].fillna('')\n",
        "\n",
        "# Remove duplicates\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Convert time\n",
        "df['unixReviewTime'] = pd.to_datetime(df['unixReviewTime'], unit='s')\n",
        "df['reviewDate'] = df['unixReviewTime'].dt.date\n",
        "\n",
        "print(\"Cleaned Data Ready\")\n"
      ],
      "metadata": {
        "id": "JsbMNj8NY_CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate review text length (number of characters)\n",
        "df['review_length'] = df['reviewText'].apply(len)\n",
        "\n",
        "# Count words in each review\n",
        "df['word_count'] = df['reviewText'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Show the first few rows with new columns\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "SQtsrT_yylvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n"
      ],
      "metadata": {
        "id": "uJu7ji9A4B9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- run this in one cell ---\n",
        "# ensure reviewText is string\n",
        "df['reviewText'] = df['reviewText'].fillna('').astype(str)\n",
        "\n",
        "# text features\n",
        "df['review_length'] = df['reviewText'].str.len()               # characters\n",
        "df['word_count'] = df['reviewText'].str.split().str.len()      # words\n",
        "df['num_exclamations'] = df['reviewText'].str.count('!')\n",
        "\n",
        "# is a very short review? (helpful flag)\n",
        "df['is_very_short'] = df['word_count'] <= 5\n",
        "\n",
        "# user activity window: days between first and last review\n",
        "user_dates = df.groupby('reviewerID')['unixReviewTime'].agg(['min','max']).reset_index()\n",
        "user_dates['active_days'] = (pd.to_datetime(user_dates['max']) - pd.to_datetime(user_dates['min'])).dt.days\n",
        "\n",
        "# Drop the existing active_days columns if they exist\n",
        "df = df.drop(columns=[col for col in df.columns if 'active_days' in col])\n",
        "\n",
        "df = df.merge(user_dates[['reviewerID','active_days']], on='reviewerID', how='left')\n",
        "\n",
        "# reviews per user (already computed as total_reviews earlier). If not, compute now:\n",
        "if 'total_reviews' not in df.columns:\n",
        "    df['total_reviews'] = df.groupby('reviewerID')['rating'].transform('count')\n",
        "\n",
        "# quick peek\n",
        "df[['reviewerID','productID','rating','word_count','review_length','num_exclamations','total_reviews','active_days']].head()"
      ],
      "metadata": {
        "id": "SMxzoW-f4XfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reviews per day (approx)\n",
        "df['reviews_per_day'] = df['total_reviews'] / (df['active_days'].replace(0,1))\n",
        "\n",
        "# Flag highly active users (thresholds you can change)\n",
        "# Example: >= 2 reviews per day on average and at least 5 reviews total\n",
        "df['flag_user_bursty'] = (df['reviews_per_day'] >= 2) & (df['total_reviews'] >= 5)\n",
        "\n",
        "# Show top flagged users\n",
        "bursty_users = df[df['flag_user_bursty']].groupby('reviewerID').agg(\n",
        "    total_reviews=('total_reviews','first'), reviews_per_day=('reviews_per_day','first')\n",
        ").sort_values('reviews_per_day', ascending=False).reset_index()\n",
        "bursty_users.head(20)"
      ],
      "metadata": {
        "id": "7vjC00kT4-fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to datetime date if not yet\n",
        "df['date'] = pd.to_datetime(df['unixReviewTime']).dt.date\n",
        "\n",
        "# daily product counts\n",
        "daily_prod = df.groupby(['productID','date']).size().reset_index(name='daily_count')\n",
        "\n",
        "# compute 7-day rolling sum per product (need a time series per product)\n",
        "daily_prod['date'] = pd.to_datetime(daily_prod['date'])\n",
        "daily_prod = daily_prod.sort_values(['productID','date'])\n",
        "\n",
        "# create 7-day rolling sum per product\n",
        "daily_prod['rolling_7d'] = daily_prod.groupby('productID')['daily_count'].transform(lambda s: s.rolling(window=7, min_periods=1).sum())\n",
        "\n",
        "# find product-days with high rolling_7d (example threshold: >= 10 reviews in 7 days)\n",
        "spikes = daily_prod[daily_prod['rolling_7d'] >= 10].sort_values(['rolling_7d'], ascending=False)\n",
        "spikes.head(20)\n"
      ],
      "metadata": {
        "id": "MpQ0Hh1H5gcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flag promotional phrases simple list\n",
        "promo_phrases = ['best product','highly recommend','five stars','works great','excellent product','buy now']\n",
        "df['lower_text'] = df['reviewText'].str.lower()\n",
        "df['has_promo_phrase'] = df['lower_text'].apply(lambda t: any(p in t for p in promo_phrases))\n",
        "\n",
        "# flag many 5-star reviews by the user\n",
        "df['user_pct_5star'] = df.groupby('reviewerID')['rating'].transform(lambda s: (s==5).mean())\n",
        "df['flag_user_always_5'] = df['user_pct_5star'] >= 0.9  # user gives >=90% five-star ratings\n",
        "\n",
        "# Example peek\n",
        "df[['reviewerID','rating','word_count','has_promo_phrase','user_pct_5star','flag_user_always_5']].head(20)\n"
      ],
      "metadata": {
        "id": "Fqn_B3PM5hVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_score(row):\n",
        "    score = 0\n",
        "    # text-based\n",
        "    if row['is_very_short']: score += 2\n",
        "    if row['has_promo_phrase']: score += 2\n",
        "    if row['num_exclamations'] >= 2: score += 1\n",
        "    # user-based\n",
        "    if row['flag_user_bursty']: score += 3\n",
        "    if row['flag_user_always_5']: score += 2\n",
        "    # product-based: if product had a 7-day spike containing this review date\n",
        "    # We'll join rolling_7d into df for quick lookup (prepare below)\n",
        "    return score\n",
        "\n",
        "# join rolling_7d back into main df to use product spike info\n",
        "daily_prod_small = daily_prod[['productID','date','rolling_7d']]\n",
        "daily_lookup = daily_prod_small.copy()\n",
        "daily_lookup['date'] = daily_lookup['date'].dt.date\n",
        "df = df.merge(daily_lookup, left_on=['productID','date'], right_on=['productID','date'], how='left')\n",
        "df['rolling_7d'] = df['rolling_7d'].fillna(0)\n",
        "\n",
        "# add product-spike flag\n",
        "df['flag_product_spike'] = df['rolling_7d'] >= 10\n",
        "\n",
        "# now compute score and add product spike weight\n",
        "df['base_score'] = df.apply(compute_score, axis=1)\n",
        "df.loc[df['flag_product_spike'], 'base_score'] += 3\n",
        "\n",
        "# create final suspicious score\n",
        "df['suspicious_score'] = df['base_score']\n",
        "\n",
        "# show top suspicious reviews\n",
        "df.sort_values('suspicious_score', ascending=False)[['reviewerID','productID','rating','reviewText','suspicious_score']].head(30)\n"
      ],
      "metadata": {
        "id": "HRt60oQ75nBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) distribution of suspicious scores\n",
        "df['suspicious_score'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Suspicious score distribution')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Number of reviews')\n",
        "plt.show()\n",
        "\n",
        "# 2) time series for a product with spikes (choose one product flagged in spikes)\n",
        "if not spikes.empty:\n",
        "    prod = spikes.iloc[0]['productID']\n",
        "    prod_ts = df[df['productID']==prod].groupby('date').size().reset_index(name='count')\n",
        "    prod_ts['date'] = pd.to_datetime(prod_ts['date'])\n",
        "    plt.plot(prod_ts['date'], prod_ts['count'])\n",
        "    plt.title(f\"Daily reviews for product {prod}\")\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No product spikes found in this dataset with current thresholds.\")\n"
      ],
      "metadata": {
        "id": "VXTj3SB55qaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top suspicious reviews (score >= threshold)\n",
        "threshold = 4\n",
        "flagged_reviews = df[df['suspicious_score'] >= threshold].sort_values('suspicious_score', ascending=False)\n",
        "flagged_reviews[['reviewerID','productID','rating','reviewText','suspicious_score']].head(50)\n",
        "\n",
        "# Top suspicious users by average suspicious_score (useful to block/quarantine)\n",
        "user_scores = df.groupby('reviewerID')['suspicious_score'].mean().reset_index().sort_values('suspicious_score', ascending=False)\n",
        "user_scores.head(30)\n"
      ],
      "metadata": {
        "id": "ItLvDi5l5v6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 0 - Install libraries\n",
        "!pip install pandas matplotlib textblob --quiet\n",
        "\n",
        "# STEP 1 - Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "\n",
        "# STEP 2 - Load dataset (make sure you uploaded small_amazon_reviews.csv in Colab)\n",
        "df = pd.read_csv(\"synthetic_amazon_reviews.csv\")\n",
        "\n",
        "# --- BASIC CLEANING ---\n",
        "df['reviewText'] = df['reviewText'].fillna('').astype(str)\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# convert timestamp to datetime\n",
        "df['unixReviewTime'] = pd.to_datetime(df['unixReviewTime'], unit='s')\n",
        "# Convert 'date' column to datetime objects before merging\n",
        "df['date'] = pd.to_datetime(df['unixReviewTime'].dt.date)\n",
        "\n",
        "df['review_length'] = df['reviewText'].str.len()\n",
        "df['word_count'] = df['reviewText'].str.split().str.len()\n",
        "df['num_exclamations'] = df['reviewText'].str.count('!')\n",
        "df['is_very_short'] = df['word_count'] <= 5\n",
        "\n",
        "# total reviews per user\n",
        "user_review_counts = df['reviewerID'].value_counts().reset_index()\n",
        "user_review_counts.columns = ['reviewerID', 'total_reviews']\n",
        "df = df.merge(user_review_counts, on='reviewerID', how='left')\n",
        "\n",
        "# average rating per user\n",
        "user_avg_rating = df.groupby('reviewerID')['rating'].mean().reset_index()\n",
        "user_avg_rating.columns = ['reviewerID', 'avg_user_rating']\n",
        "df = df.merge(user_avg_rating, on='reviewerID', how='left')\n",
        "\n",
        "# user active days\n",
        "user_dates = df.groupby('reviewerID')['unixReviewTime'].agg(['min','max']).reset_index()\n",
        "user_dates['active_days'] = (pd.to_datetime(user_dates['max']) - pd.to_datetime(user_dates['min'])).dt.days\n",
        "df = df.merge(user_dates[['reviewerID','active_days']], on='reviewerID', how='left')\n",
        "df['reviews_per_day'] = df['total_reviews'] / (df['active_days'].replace(0,1))\n",
        "\n",
        "# flag bursty users (>=2 reviews/day & >=5 total)\n",
        "df['flag_user_bursty'] = (df['reviews_per_day'] >= 2) & (df['total_reviews'] >= 5)\n",
        "\n",
        "daily_prod = df.groupby(['productID','date']).size().reset_index(name='daily_count')\n",
        "daily_prod['date'] = pd.to_datetime(daily_prod['date'])\n",
        "daily_prod = daily_prod.sort_values(['productID','date'])\n",
        "daily_prod['rolling_7d'] = daily_prod.groupby('productID')['daily_count'].transform(lambda s: s.rolling(window=7, min_periods=1).sum())\n",
        "spikes = daily_prod[daily_prod['rolling_7d'] >= 10]\n",
        "df = df.merge(daily_prod[['productID','date','rolling_7d']], on=['productID','date'], how='left')\n",
        "df['rolling_7d'] = df['rolling_7d'].fillna(0)\n",
        "df['flag_product_spike'] = df['rolling_7d'] >= 10\n",
        "\n",
        "\n",
        "promo_phrases = ['best product','highly recommend','five stars','works great','excellent product','buy now']\n",
        "df['lower_text'] = df['reviewText'].str.lower()\n",
        "df['has_promo_phrase'] = df['lower_text'].apply(lambda t: any(p in t for p in promo_phrases))\n",
        "df['user_pct_5star'] = df.groupby('reviewerID')['rating'].transform(lambda s: (s==5).mean())\n",
        "df['flag_user_always_5'] = df['user_pct_5star'] >= 0.9\n",
        "\n",
        "def compute_score(row):\n",
        "    score = 0\n",
        "    if row['is_very_short']: score += 2\n",
        "    if row['has_promo_phrase']: score += 2\n",
        "    if row['num_exclamations'] >= 2: score += 1\n",
        "    if row['flag_user_bursty']: score += 3\n",
        "    if row['flag_user_always_5']: score += 2\n",
        "    if row['flag_product_spike']: score += 3\n",
        "    return score\n",
        "\n",
        "df['suspicious_score'] = df.apply(compute_score, axis=1)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "df['suspicious_score'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Suspicious Score Distribution')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.show()\n",
        "\n",
        "# show one product's daily review pattern (if spike exists)\n",
        "if not spikes.empty:\n",
        "    prod = spikes.iloc[0]['productID']\n",
        "    prod_ts = df[df['productID']==prod].groupby('date').size().reset_index(name='count')\n",
        "    prod_ts['date'] = pd.to_datetime(prod_ts['date'])\n",
        "    plt.plot(prod_ts['date'], prod_ts['count'])\n",
        "    plt.title(f\"Daily Reviews for Product {prod}\")\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No product spikes detected with current settings.\")\n",
        "\n",
        "\n",
        "print(\"\\nTop suspicious reviews (score >= 4):\")\n",
        "flagged_reviews = df[df['suspicious_score'] >= 4].sort_values('suspicious_score', ascending=False)\n",
        "print(flagged_reviews[['reviewerID','productID','rating','reviewText','suspicious_score']].head(15))\n",
        "\n",
        "print(\"\\nTop suspicious users:\")\n",
        "user_scores = df.groupby('reviewerID')['suspicious_score'].mean().reset_index().sort_values('suspicious_score', ascending=False)\n",
        "print(user_scores.head(15))\n",
        "\n",
        "print(\"\\nâœ… Analysis complete!\")"
      ],
      "metadata": {
        "id": "erL3FNTa6aP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ðŸŸ¢ replace the path inside read_csv with your copied path\n",
        "df = pd.read_csv(\"/content/synthetic_amazon_reviews.csv\")\n",
        "\n",
        "# Show first few rows to confirm load\n",
        "display(df.head())\n",
        "\n",
        "# Basic info about columns\n",
        "df.info()\n",
        "\n",
        "# Summary stats\n",
        "print(\"Total reviews:\", len(df))\n",
        "print(\"Unique products:\", df['productID'].nunique())\n",
        "print(\"Unique users:\", df['reviewerID'].nunique())\n",
        "\n",
        "# Rating distribution\n",
        "df['rating'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Rating Distribution')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6WnvkVyvJ5wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0X5Egxb55sZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count how many reviews each product gets per day\n",
        "daily = df.groupby(['productID', 'reviewDate']).size().reset_index(name='daily_count')\n",
        "\n",
        "#Sort by product and date so we can check recent activity\n",
        "daily['reviewDate'] = pd.to_datetime(daily['reviewDate'])\n",
        "daily = daily.sort_values(['productID', 'reviewDate'])\n",
        "\n",
        "#Compute a 7-day rolling window for each product\n",
        "daily['rolling_7d'] = daily.groupby('productID')['daily_count'].transform(\n",
        "    lambda s: s.rolling(7, min_periods=1).sum()\n",
        ")\n",
        "\n",
        "#Drop existing rolling_7d columns before merging to avoid conflicts\n",
        "df = df.drop(columns=[col for col in df.columns if 'rolling_7d' in col])\n",
        "\n",
        "#Merge this info back into main dataframe\n",
        "daily2 = daily[['productID','reviewDate','rolling_7d']].copy()\n",
        "# daily2['reviewDate'] = daily2['reviewDate'].dt.date # Removed .dt.date conversion\n",
        "df = df.merge(daily2, on=['productID','reviewDate'], how='left')\n",
        "\n",
        "#Flag if the product had â‰¥10 reviews in any 7-day period\n",
        "df['rolling_7d'] = df['rolling_7d'].fillna(0)\n",
        "df['flag_product_spike'] = df['rolling_7d'] >= 10  # adjust threshold if dataset is small\n",
        "\n",
        "#Create a function that adds up all suspicious signals\n",
        "def score_row(r):\n",
        "    score = 0\n",
        "    # review-level clues\n",
        "    if r['is_very_short']: score += 2\n",
        "    if r['has_promo_phrase']: score += 2\n",
        "    if r['num_exclamations'] >= 2: score += 1\n",
        "    # user-level clues\n",
        "    if r['flag_user_bursty']: score += 3\n",
        "    if r['flag_user_always_5']: score += 2\n",
        "    # product-level clue\n",
        "    if r['flag_product_spike']: score += 3\n",
        "    return score\n",
        "\n",
        "#Apply the scoring function to every review\n",
        "df['suspicious_score'] = df.apply(score_row, axis=1)\n",
        "\n",
        "#See the top 15 most suspicious reviews\n",
        "display(\n",
        "    df.sort_values('suspicious_score', ascending=False)[\n",
        "        ['reviewerID','productID','rating','reviewText','suspicious_score']\n",
        "    ].head(15)\n",
        ")"
      ],
      "metadata": {
        "id": "R9pXKxDq4GJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db8bdef7"
      },
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/synthetic_amazon_reviews.csv\")\n",
        "\n",
        "# Basic cleaning\n",
        "df['reviewText'] = df['reviewText'].fillna('').astype(str)\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Convert unixReviewTime to datetime and extract date\n",
        "df['unixReviewTime'] = pd.to_datetime(df['unixReviewTime'], unit='s')\n",
        "df['reviewDate'] = pd.to_datetime(df['unixReviewTime'].dt.date) # Ensure reviewDate is datetime object\n",
        "\n",
        "# Basic text-based features\n",
        "df['review_length'] = df['reviewText'].str.len()          # number of characters\n",
        "df['word_count'] = df['reviewText'].str.split().str.len() # number of words\n",
        "df['num_exclamations'] = df['reviewText'].str.count('!')  # count exclamation marks\n",
        "df['is_very_short'] = df['word_count'] <= 5\n",
        "\n",
        "# User-based features\n",
        "df['total_reviews'] = df.groupby('reviewerID')['rating'].transform('count')\n",
        "user_dates = df.groupby('reviewerID')['unixReviewTime'].agg(['min', 'max']).reset_index()\n",
        "user_dates['active_days'] = (pd.to_datetime(user_dates['max']) - pd.to_datetime(user_dates['min'])).dt.days\n",
        "df = df.merge(user_dates[['reviewerID', 'active_days']], on='reviewerID', how='left')\n",
        "df['reviews_per_day'] = df['total_reviews'] / (df['active_days'].replace(0,1))\n",
        "df['flag_user_bursty'] = (df['reviews_per_day'] >= 2) & (df['total_reviews'] >= 5)\n",
        "df['user_pct_5star'] = df.groupby('reviewerID')['rating'].transform(lambda s: (s==5).mean())\n",
        "df['flag_user_always_5'] = df['user_pct_5star'] >= 0.9\n",
        "\n",
        "\n",
        "# Product-based features (Spikes)\n",
        "daily = df.groupby(['productID', 'reviewDate']).size().reset_index(name='daily_count')\n",
        "daily['reviewDate'] = pd.to_datetime(daily['reviewDate'])\n",
        "daily = daily.sort_values(['productID', 'reviewDate'])\n",
        "daily['rolling_7d'] = daily.groupby('productID')['daily_count'].transform(\n",
        "    lambda s: s.rolling(7, min_periods=1).sum()\n",
        ")\n",
        "daily2 = daily[['productID','reviewDate','rolling_7d']].copy()\n",
        "# No need to convert to .dt.date here as reviewDate is already datetime\n",
        "df = df.merge(daily2, on=['productID','reviewDate'], how='left')\n",
        "df['rolling_7d'] = df['rolling_7d'].fillna(0)\n",
        "df['flag_product_spike'] = df['rolling_7d'] >= 10\n",
        "\n",
        "\n",
        "# Text-based features (Promo phrases)\n",
        "promo_phrases = ['best product','highly recommend','five stars','works great','excellent product','buy now']\n",
        "df['lower_text'] = df['reviewText'].str.lower()\n",
        "df['has_promo_phrase'] = df['lower_text'].apply(lambda t: any(p in t for p in promo_phrases))\n",
        "\n",
        "\n",
        "# Create a function that adds up all suspicious signals\n",
        "def score_row(r):\n",
        "    score = 0\n",
        "    # review-level clues\n",
        "    if r['is_very_short']: score += 2\n",
        "    if r['has_promo_phrase']: score += 2\n",
        "    if r['num_exclamations'] >= 2: score += 1\n",
        "    # user-level clues\n",
        "    if r['flag_user_bursty']: score += 3\n",
        "    if r['flag_user_always_5']: score += 2\n",
        "    # product-level clue\n",
        "    if r['flag_product_spike']: score += 3\n",
        "    return score\n",
        "\n",
        "# Apply the scoring function to every review\n",
        "df['suspicious_score'] = df.apply(score_row, axis=1)\n",
        "\n",
        "# See the top 15 most suspicious reviews\n",
        "display(\n",
        "    df.sort_values('suspicious_score', ascending=False)[\n",
        "        ['reviewerID','productID','rating','reviewText','suspicious_score']\n",
        "    ].head(15)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count how many reviews each product gets per day\n",
        "daily = df.groupby(['productID', 'reviewDate']).size().reset_index(name='daily_count')\n",
        "\n",
        "#Sort by product and date so we can check recent activity\n",
        "daily['reviewDate'] = pd.to_datetime(daily['reviewDate'])\n",
        "daily = daily.sort_values(['productID', 'reviewDate'])\n",
        "\n",
        "#Compute a 7-day rolling window for each product\n",
        "daily['rolling_7d'] = daily.groupby('productID')['daily_count'].transform(\n",
        "    lambda s: s.rolling(7, min_periods=1).sum()\n",
        ")\n",
        "\n",
        "#Convert df['reviewDate'] to datetime before merging\n",
        "df['reviewDate'] = pd.to_datetime(df['reviewDate'])\n",
        "\n",
        "#Merge this info back into main dataframe\n",
        "daily2 = daily[['productID','reviewDate','rolling_7d']].copy()\n",
        "# Remove the .dt.date conversion here to keep reviewDate as datetime\n",
        "# daily2['reviewDate'] = daily2['reviewDate'].dt.date # This line is now commented out in the previous modification\n",
        "\n",
        "df = df.merge(daily2, on=['productID','reviewDate'], how='left')\n",
        "\n",
        "#Flag if the product had â‰¥10 reviews in any 7-day period\n",
        "df['rolling_7d'] = df['rolling_7d'].fillna(0)\n",
        "df['flag_product_spike'] = df['rolling_7d'] >= 10  # adjust threshold if dataset is small\n",
        "\n",
        "#Create a function that adds up all suspicious signals\n",
        "def score_row(r):\n",
        "    score = 0\n",
        "    # review-level clues\n",
        "    if r['is_very_short']: score += 2\n",
        "    if r['has_promo_phrase']: score += 2\n",
        "    if r['num_exclamations'] >= 2: score += 1\n",
        "    # user-level clues\n",
        "    if r['flag_user_bursty']: score += 3\n",
        "    if r['flag_user_always_5']: score += 2\n",
        "    # product-level clue\n",
        "    if r['flag_product_spike']: score += 3\n",
        "    return score\n",
        "\n",
        "#Apply the scoring function to every review\n",
        "df['suspicious_score'] = df.apply(score_row, axis=1)\n",
        "\n",
        "#See the top 15 most suspicious reviews\n",
        "display(\n",
        "    df.sort_values('suspicious_score', ascending=False)[\n",
        "        ['reviewerID','productID','rating','reviewText','suspicious_score']\n",
        "    ].head(15)\n",
        ")"
      ],
      "metadata": {
        "id": "Sv1fGRHj6nU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rating Distribution\n",
        "df['rating'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Rating Distribution')\n",
        "plt.xlabel('Rating (Stars)')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.show()\n",
        "\n",
        "#Suspicious Score Distribution\n",
        "df['suspicious_score'].value_counts().sort_index().plot(kind='bar')\n",
        "plt.title('Suspicious Score Distribution')\n",
        "plt.xlabel('Suspicious Score')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e-sGlWJR7UFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = # choose what \"suspicious\" means\n",
        "flagged_reviews = df[df['suspicious_score'] >= threshold]\n",
        "print(\"Total flagged reviews:\", len(flagged_reviews))\n",
        "display(flagged_reviews[['reviewerID','productID','rating','reviewText','suspicious_score']].head(20))\n",
        "\n",
        "# Top suspicious users\n",
        "user_scores = df.groupby('reviewerID')['suspicious_score'].mean().reset_index().sort_values('suspicious_score', ascending=False)\n",
        "display(user_scores.head(10))"
      ],
      "metadata": {
        "id": "Odo2bX1T8Md9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}